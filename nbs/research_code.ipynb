{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# research_code\n",
    "\n",
    "> A selection of useful code chunks for analysing THz-TDS data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from fastcore.test import *\n",
    "import fastcore.docments as docment\n",
    "from nbdev.showdoc import DocmentTbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp research_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "import numpy as np\n",
    "from scipy import constants as consts\n",
    "from scipy.optimize import curve_fit\n",
    "from scipy.stats import chisquare\n",
    "from scipy.fft import fft,fftfreq,rfft, fftshift,fftn\n",
    "from scipy.interpolate import interp1d\n",
    "import inspect\n",
    "import pandas as pd\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import glob,os,re,pathlib\n",
    "import seaborn as sns\n",
    "from scipy.signal import savgol_filter\n",
    "from alive_progress import alive_it, alive_bar\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def exp_decay(x: np.ndarray, # Input array that contains the indpendent variable.\n",
    "    Amplitude: np.single, # Pre-factor of the exponential decay. \n",
    "    tau: np.single, # Decay constant.\n",
    "    x0: np.single, # Offset for the independent variable .\n",
    "    ) -> np.ndarray: # A new float array containinsg the exponentially decaying function.\n",
    "    '''\n",
    "    Single exponentially decaying function.\n",
    "    '''\n",
    "    decayOutput = Amplitude * np.exp(-(x-x0)/tau)\n",
    "    \n",
    "    return decayOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def biexp_decay_beta(x:np.ndarray, # Input array that contains the indpendent variable.\n",
    "    Amplitude1: np.single, # Prefactor of the first exponential decay.\n",
    "    tau1: np.single, # Decay constant of the first exponential term.\n",
    "    beta: np.single, # Ratio that decays via channel1 or channel2\n",
    "    tau2: np.single, # Decay constant of the second exponential term.\n",
    "    x0: np.single, # Offset for the indpendent variable.\n",
    "    ) -> np.ndarray: # A new float array containing the exponentially decaying function.\n",
    "    '''\n",
    "    Bi-exponentially decaying function.\n",
    "    '''\n",
    "    decayOutput = Amplitude1 * (beta * np.exp(-(x-x0)/tau1) + (1 - beta) * np.exp(-(x-x0)/tau2))\n",
    "    \n",
    "    return decayOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def biexp_decay(x:np.ndarray, # Input array that contains the indpendent variable.\n",
    "    Amplitude1: np.single, # Prefactor of the first exponential decay.\n",
    "    tau1: np.single, # Decay constant of the first exponential term.\n",
    "    Amplitude2: np.single, # Prefactor of the second exponential decay.\n",
    "    tau2: np.single, # Decay constant of the second exponential term.\n",
    "    x0: np.single, # Offset for the indpendent variable.\n",
    "    ) -> np.ndarray: # A new float array containing the exponentially decaying function.\n",
    "    '''\n",
    "    Bi-exponentially decaying function.\n",
    "    '''\n",
    "    decayOutput = Amplitude1 * np.exp(-(x-x0)/tau1) + Amplitude2 * np.exp(-(x-x0)/tau2)\n",
    "    \n",
    "    return decayOutput"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gaussian(x:np.ndarray, # Input array that contains the indpendent variable.\n",
    "    Amplitude: np.single, # Prefactor of the gaussian.\n",
    "    sigma: np.single, # Standard deviation of the gaussian.\n",
    "    x0: np.single, # Offset for the indpendent variable.\n",
    "    ) -> np.ndarray: # A new float array containing the gaussian function.\n",
    "    '''\n",
    "    Gaussian function.\n",
    "    '''\n",
    "    outputGaussian = Amplitude * np.exp(-((x-x0)/sigma)**2)\n",
    "\n",
    "    return outputGaussian"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convolved_exp_decay(x:np.ndarray, # Input array that contains the indpendent variable.\n",
    "    Amplitude: np.single, # Prefactor of the exponential decay.\n",
    "    tau: np.single, # Decay constant.\n",
    "    sigma: np.single, # Standard deviation of the gaussian\n",
    "    x0: np.single, # Offset for the indpendent variable.\n",
    "    y0: np.single, # Offset in y.\n",
    "    ) -> np.ndarray: # A new float array containing the exponentially decaying function.\n",
    "    '''\n",
    "    Single exponentially decaying function convolved with a gaussian.\n",
    "    '''\n",
    "    L = len(x) # length of the independent variable.\n",
    "\n",
    "    exp_term = exp_decay(x,1,tau,x0) # The exponential decay.\n",
    "\n",
    "    gaussian_term = gaussian(x,1.0,sigma,x0) # The gaussian part, setting the amplitude normalised to 1.0.\n",
    "    \n",
    "    convolved = np.convolve(exp_term,gaussian_term,mode='full')[0:L] # convolved function.\n",
    "\n",
    "    convolved = Amplitude*convolved/np.max(convolved) # Normalise the function so the maximum is at Amplitude\n",
    "    \n",
    "    return convolved + y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convolved_biexp_decay(x:np.ndarray, # Input array that contains the indpendent variable.\n",
    "    Amplitude1:np.single, # Prefactor of the first exponential decay.\n",
    "    tau1:np.single, # Decay constant of the first exponential term.\n",
    "    Amplitude2:np.single, # Prefactor of the second exponential decay.\n",
    "    tau2:np.single, # Decay constant of the second exponential term.\n",
    "    sigma:np.single, # Standard deviation of the gaussian\n",
    "    x0:np.single, # Offset for the indpendent variable.\n",
    "    y0:np.single, # Offset in y.\n",
    "    ) ->np.ndarray: # A new float array containing the exponentially decaying function.\n",
    "    '''\n",
    "    Bi-exponentially decaying function convolved with a gaussian.\n",
    "    '''    \n",
    "    L = len(x) # length of the independent variable.\n",
    "\n",
    "    exp_term1 = exp_decay(x,1,tau1,x0) # The biexponential decay.\n",
    "\n",
    "    exp_term2 = exp_decay(x,1,tau2,x0) # The biexponential decay.\n",
    "\n",
    "    gaussian_term = gaussian(x,1.0,sigma,x0) # The gaussian part, setting the amplitude normalised to 1.0.\n",
    "    \n",
    "    convolved1 = np.convolve(gaussian_term,exp_term1,mode='full')[0:L]\n",
    "    convolved2 = np.convolve(gaussian_term,exp_term2,mode='full')[0:L]# convolved function.\n",
    "\n",
    "    convolved = Amplitude1*convolved1/np.max(convolved1) + Amplitude2*convolved2/np.max(convolved2)\n",
    "\n",
    "    #convolved *= (Amplitude1+Amplitude2-y0)/np.max(convolved) # Normalise the function so the maximum is at Amplitude\n",
    "    \n",
    "    return convolved + y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def convolved_biexp_decay_beta(x:np.ndarray, # Input array that contains the indpendent variable.\n",
    "    Amplitude1:np.single, # Prefactor of the first exponential decay.\n",
    "    tau1:np.single, # Decay constant of the first exponential term.\n",
    "    beta:np.single, # Prefactor of the second exponential decay.\n",
    "    tau2:np.single, # Decay constant of the second exponential term.\n",
    "    sigma:np.single, # Standard deviation of the gaussian\n",
    "    x0:np.single, # Offset for the indpendent variable.\n",
    "    y0:np.single, # Offset in y.\n",
    "    ) ->np.ndarray: # A new float array containing the exponentially decaying function.\n",
    "    '''\n",
    "    Bi-exponentially decaying function, with a beta term, convolved with a gaussian.\n",
    "    '''    \n",
    "    L = len(x) # length of the independent variable.\n",
    "\n",
    "    gaussian_term = gaussian(x,1.0,sigma,x0) # The gaussian part, setting the amplitude normalised to 1.0.\n",
    "    \n",
    "    exp_term1 = np.exp(-(x-x0)/tau1)\n",
    "\n",
    "    exp_term2 = np.exp(-(x-x0)/tau2)\n",
    "\n",
    "    convolved_term1 = np.convolve(gaussian_term,exp_term1,mode='full')[0:L]\n",
    "\n",
    "    convolved_term2 = np.convolve(gaussian_term,exp_term2,mode='full')[0:L]\n",
    "    \n",
    "    convolved = beta * convolved_term1/np.max(convolved_term1)\n",
    "\n",
    "    convolved += (1 - beta) * convolved_term2/np.max(convolved_term2)\n",
    "    \n",
    "    return convolved + y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def rebin(x,y,m):\n",
    "    x = np.array(x)\n",
    "    y = np.array(y)\n",
    "    xnew = np.linspace(x[0],x[-1],int(round(len(x)/m)*m))\n",
    "    yinterp = np.interp(xnew, x, y)\n",
    "    \n",
    "    window_sz = m\n",
    "    ym = yinterp[:int(len(yinterp)/window_sz*window_sz)].reshape(-1,window_sz).mean(1)\n",
    "    xm = np.linspace(xnew[0],xnew[-1],int(len(xnew)/m))\n",
    "\n",
    "    return (xm,ym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def R2(xdata,ydata,residual):\n",
    "        ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "        ss_res = np.sum(residual**2)\n",
    "        return (1 - ss_res/ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def R2_adj(xdata,ydata,residual,popt):\n",
    "        N_points = len(xdata)\n",
    "        nos_fit_params = len(popt)\n",
    "        ss_tot = np.sum((ydata-np.mean(ydata))**2)\n",
    "        ss_res = np.sum(residual**2)\n",
    "        R2 = 1 - ss_res/ss_tot\n",
    "        R2_adj = 1 - (1 - R2)*(N_points-1)/(N_points-nos_fit_params-1)\n",
    "        return R2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def X2_adj(xdata,ydata,residual,named_function,popt):\n",
    "    N_points = len(xdata)\n",
    "    nos_fit_params = len(popt)\n",
    "    rss = np.sum(residual**2)\n",
    "    x2_adj = rss / (N_points-nos_fit_params)\n",
    "    return x2_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def X2(xdata,ydata,residual,named_function,popt):\n",
    "    expected_values = named_function(xdata,*popt)\n",
    "    return np.sum((1/expected_values) * residual**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def R_standardised(xdata,ydata,residual,named_function,popt):\n",
    "\n",
    "    N_points = len(xdata)\n",
    "    \n",
    "    nos_fit_params = len(popt)\n",
    "    \n",
    "    RSS = np.sum(residual**2)\n",
    "\n",
    "    RSE = np.sqrt(RSS/(N_points - nos_fit_params - 1))\n",
    "\n",
    "    X = np.vstack([residual,np.ones(len(residual))]).T\n",
    "\n",
    "    h_leverage = np.diag(X @ np.linalg.inv(X.T @ X) @ X.T)\n",
    "\n",
    "    r_standardised = residual/np.sqrt(RSE*np.sqrt(1-h_leverage))\n",
    "\n",
    "    return r_standardised"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class fit_result:\n",
    "    def __init__(self,xdata,ydata,named_function,popt,pcov):\n",
    "        self.popt = popt\n",
    "        self.pstd = np.sqrt(np.diag(pcov))\n",
    "        self.residual = ydata-named_function(xdata,*popt)\n",
    "        self.R2 = R2(xdata,ydata,self.residual)\n",
    "        self.R2_adj = R2_adj(xdata,ydata,self.residual,popt)\n",
    "        self.RMSE = np.sqrt(np.sum(self.residual**2) / (len(xdata) - len(popt)))\n",
    "        self.chi2_adj = X2_adj(xdata,ydata,self.residual,named_function,popt)\n",
    "        self.r_standardised = R_standardised(xdata,ydata,self.residual,named_function,popt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def interp_data(x:np.ndarray, # independent variable\n",
    "                y:np.ndarray, # dependent variable\n",
    "               ): # returns interpolated data for minimum delta\n",
    "    interp_f = interp1d(x,y)\n",
    "    xx_new = np.arange(np.min(x),np.max(x),np.diff(x).min()*1)\n",
    "    yy_new = interp_f(xx_new)\n",
    "\n",
    "    return (xx_new,yy_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fit_curve(named_function, # the fit function\n",
    "              x:np.ndarray, # independent variable \n",
    "              y:np.ndarray, # dependent variable\n",
    "              **kwargs) -> object: # returns a an object of the class 'fit_object'\n",
    "\n",
    "    (xnew,ynew) = interp_data(x,y)\n",
    "    \n",
    "    if 'guess' in list(kwargs.keys()):\n",
    "        guess = kwargs['guess']\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "    if 'method' in list(kwargs.keys()):\n",
    "        method = kwargs['method']\n",
    "    else:\n",
    "        method = 'lm'\n",
    "        \n",
    "    def fmod(named_function,x,*args,**kwargs):\n",
    "\n",
    "        all_args = inspect.getfullargspec(named_function).args\n",
    "        sub_args = list(kwargs.keys()) # just the names of the fixed variables\n",
    "        sub_vals = list(kwargs.values())\n",
    "    \n",
    "        all_args.pop(0)\n",
    "        new_kwargs = {}\n",
    "        truth = tuple(val in sub_args for val in all_args)\n",
    "        #truth = np.logical_not(truth)\n",
    "        j=0\n",
    "        largs = list(args)\n",
    "        \n",
    "        for i,arg in enumerate(all_args):\n",
    "            if truth[i]==True:\n",
    "                new_kwargs[arg] = kwargs[arg]\n",
    "            elif truth[i]==False:\n",
    "                new_kwargs[arg] = largs[j]\n",
    "                # print(largs[j])\n",
    "                j+=1\n",
    "                \n",
    "        return named_function(x,**new_kwargs)\n",
    "    \n",
    "    all_args = np.array(named_function.__code__.co_varnames)\n",
    "\n",
    "    ([*keys],vals) = zip(*guess.items())\n",
    "    \n",
    "    guess2,truth = np.ndarray.flatten(np.array(vals).T).reshape(2,len(keys))\n",
    "\n",
    "    # print(guess2,truth)\n",
    "\n",
    "    truth = truth.astype(int)\n",
    "    not_truth = np.logical_not(truth).astype(int)\n",
    "\n",
    "    key_mask = np.ma.masked_array(keys,mask=not_truth).compressed()\n",
    "    guess_mask = np.ma.masked_array(guess2,mask=not_truth).compressed() # the fixed values\n",
    "    guess_mask_float = np.ma.masked_array(guess2,mask=truth).compressed() # the non-fixed values\n",
    "\n",
    "    kwargs = dict(zip(key_mask,guess_mask))\n",
    "\n",
    "    fun2 = lambda x,*args: fmod(named_function,x,*args,**kwargs) \n",
    "    \n",
    "    popt,pcov = curve_fit(fun2,xnew,ynew,p0=guess_mask_float,method=method)\n",
    "\n",
    "    fit_object = fit_result(x,y,fun2,popt,pcov)\n",
    "    \n",
    "    pstd = np.sqrt(np.diag(pcov))\n",
    "\n",
    "    popt_new = np.zeros(len(guess2))\n",
    "    pstd_new = np.zeros(len(guess2))\n",
    "\n",
    "    j=0\n",
    "    k=0\n",
    "    for i,val in enumerate(truth):\n",
    "        if val==False:\n",
    "            popt_new[i] = popt[j]\n",
    "            pstd_new[i] = pstd[j]\n",
    "            j+=1\n",
    "        else:\n",
    "            popt_new[i] = guess_mask[k]\n",
    "            pstd_new[i] = 0.0\n",
    "            k+=1\n",
    "\n",
    "    fit_object.popt = popt_new\n",
    "    fit_object.pstd = pstd_new\n",
    "    return fit_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fit_trace2(x:np.ndarray, # the independent variable (e.g. x,time), usually time here\n",
    "              y:np.ndarray, # the dependent variable (e.g. y)\n",
    "              trange:tuple, # the x range to fit the data over\n",
    "              guess:dict, # a dictionary containing the guess parameters for the fit\n",
    "              named_function, # the fit function\n",
    "             ) -> tuple:\n",
    "\n",
    "    tol = np.abs(x[1]-x[0])\n",
    "    t_start = np.where(np.abs(x-trange[0])<tol)[0][0]\n",
    "    t_end = np.where(np.abs(x-trange[1])<tol)[0][0]\n",
    "    \n",
    "    x_sub = x[t_start:t_end]\n",
    "    y_sub = y[t_start:t_end]\n",
    "    \n",
    "    popt,pcov = curve_fit(named_function,x_sub,y_sub,p0=guess,method='lm')\n",
    "\n",
    "    fit_object = fit_result(x_sub,y_sub,named_function,popt,pcov)\n",
    "    \n",
    "    return fit_object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def global_fit(named_function, # the name of the function being fitted\n",
    "               x:dict, # Input array that contains the indpendent variable\n",
    "               y:dict, # dependent variable, can be a matrix\n",
    "               guess:dict, # guess parameters in a dictionary, with gobal as True or false\n",
    "              ) ->np.ndarray: # A new float array containing the exponentially decaying function.\n",
    "\n",
    "    '''\n",
    "    The purpose of this function is to allow for the simple global fitting of data\n",
    "    '''\n",
    "    \n",
    "    ([*keys],vals) = zip(*guess.items())\n",
    "    \n",
    "    guess2,truth_global = np.ndarray.flatten(np.array(vals).T).reshape(2,len(keys))\n",
    "    \n",
    "    N = len(guess2) # the full number of guess values\n",
    "\n",
    "    ([*keys_x],[*vals_x])=zip(*x.items())\n",
    "    ([*keys_y],[*vals_y])=zip(*y.items())\n",
    "\n",
    "    M = len(keys_x)\n",
    "    L = [len(vals_x[i]) for i in range(len(keys_x))]\n",
    "\n",
    "    # print(f'This had better be OK: {L}')\n",
    "    \n",
    "    \n",
    "    # M,L = np.shape(y) # will give the number of points in each plot (L) and the number of different traces (M). This assumes that there is the same\n",
    "    # number of points in each run...\n",
    "\n",
    "    for i in range(M):\n",
    "        if i==0:\n",
    "            new_x = np.array(vals_x[i])\n",
    "            new_y = np.array(vals_y[i])\n",
    "        else:\n",
    "            new_x = np.concatenate([new_x,vals_x[i]])\n",
    "            new_y = np.concatenate([new_y,vals_y[i]])\n",
    "    #     new_x = np.ndarray.flatten(np.array(vals_x)) # will give an x vector of same length as new_y\n",
    "    # new_y = np.ndarray.flatten(np.array(vals_y)) # flatten all the data into one long column vector\n",
    "\n",
    "    all_args = inspect.getfullargspec(named_function).args\n",
    "\n",
    "    N_args = len(all_args) - 1\n",
    "\n",
    "    truth_global = truth_global.astype(int) #\n",
    "\n",
    "    \n",
    "    for j in range(N_args):\n",
    "        if truth_global[j]==True:\n",
    "            truth_global[j] = False\n",
    "            for k in range(1,M):\n",
    "                truth_global[j+k*N_args]=True\n",
    "        else:\n",
    "            pass\n",
    "        \n",
    "    truth_local = np.logical_not(truth_global).astype(int)\n",
    "\n",
    "    key_mask_local = np.ma.masked_array(keys,mask=truth_global).compressed()\n",
    "    guess_mask_local = np.ma.masked_array(guess2,mask=truth_global).compressed() # the unfixed values\n",
    "    guess_mask_global = np.ma.masked_array(guess2,mask=truth_local).compressed() # the fixed values\n",
    "\n",
    "    first_vals = guess2[:N_args]\n",
    "\n",
    "    def builder(x,*args):\n",
    "        \n",
    "        ypiece = np.zeros(len(x))\n",
    "\n",
    "        #print(f'M: {M}')\n",
    "        args2 = np.zeros(N_args)\n",
    "        Nargs = len(args)\n",
    "        v=0\n",
    "        #print(L)\n",
    "        for i in range(M):\n",
    "\n",
    "            for j,val in enumerate(truth_local[i*N_args:(i+1)*N_args]):\n",
    "\n",
    "                if val==0:\n",
    "                    args2[j] = args[j]\n",
    "                    \n",
    "                else:\n",
    "                    args2[j] = args[v]\n",
    "                    v+=1\n",
    "            if i==0:\n",
    "                start_count = 0\n",
    "                end_count = L[i]\n",
    "            else:\n",
    "                start_count = int(np.sum(L[0:i])+1)\n",
    "                end_count = int(np.sum(L[0:i+1]))\n",
    "                # print(start_count,end_count)\n",
    "            \n",
    "            ypiece[start_count:end_count] = named_function(x[start_count:end_count],*args2)\n",
    "            #print(f'ya: {ypiece}')\n",
    "        return ypiece\n",
    "\n",
    "    if named_function==convolved_biexp_decay_beta:\n",
    "        L_beta = N_args\n",
    "        M_beta = len(truth_global)\n",
    "        \n",
    "        b_low = -np.inf * np.ones(M_beta)\n",
    "        b_high = np.inf * np.ones(M_beta)\n",
    "        \n",
    "        for j in range(2,M_beta,L_beta):\n",
    "            b_low[j] = 0\n",
    "            b_high[j] = 1\n",
    "\n",
    "        for j in range(1,M_beta,L_beta):\n",
    "            b_low[j] = 0\n",
    "\n",
    "        for j in range(3,M_beta,L_beta):\n",
    "            b_low[j] = 0\n",
    "        \n",
    "        bounds_lower = np.ma.masked_array(b_low,mask=truth_global).compressed()\n",
    "        bounds_upper = np.ma.masked_array(b_high,mask=truth_global).compressed()\n",
    "\n",
    "        print(guess_mask_local,bounds_lower,bounds_upper)\n",
    "        popt,pcov = curve_fit(builder,new_x,new_y,p0=guess_mask_local,bounds=(bounds_lower,bounds_upper),method='trf')\n",
    "        \n",
    "    else:\n",
    "        popt,pcov = curve_fit(builder,new_x,new_y,p0=guess_mask_local,method='lm')\n",
    "\n",
    "    fit_object = fit_result(new_x,new_y,builder,popt,pcov)\n",
    "\n",
    "    pstd = np.sqrt(np.diag(pcov))\n",
    "\n",
    "    popt_new = np.zeros(len(guess2))\n",
    "    pstd_new = np.zeros(len(guess2))\n",
    "\n",
    "    j=0\n",
    "    for i,val in enumerate(truth_global):\n",
    "        if val==False:\n",
    "            popt_new[i] = popt[j]\n",
    "            pstd_new[i] = pstd[j]\n",
    "            j+=1\n",
    "        else:\n",
    "            popt_new[i] = popt[i-(i//N_args)*N_args]\n",
    "            pstd_new[i] = 0.0\n",
    "\n",
    "    popt_new = np.reshape(popt_new,(M,N_args))\n",
    "\n",
    "    pstd_new = np.reshape(pstd_new,(M,N_args))\n",
    "\n",
    "    fit_object.popt = popt_new\n",
    "\n",
    "    fit_object.pstd = pstd_new\n",
    "\n",
    "    trace_new = ['trace'+str(i) for i in range(M)]\n",
    "\n",
    "    dict_test = {'trace'+str(i):[(round(popt_new[i,j],4),round(pstd_new[i,j],4)) for j in range(N_args)] for i in range(M)}\n",
    "\n",
    "    df = pd.DataFrame(dict_test)\n",
    "    df['name'] = all_args[1:]\n",
    "    # test = dict(zip(keys,popt_new))\n",
    "    df.set_index('name',inplace=True)\n",
    "    df.index.name = None\n",
    "    df['Global_var'] = truth_global[N_args:2*N_args].astype(bool)\n",
    "\n",
    "    # for i in range(M):\n",
    "    #     R2 = df._append(new_row, ignore_index=True) ## Do more here to add anotehr Row with the RMSE/R2 for each trace\n",
    "    # df['newlevel'] = 'C'\n",
    "    # df = df.set_index('newlevel', append=True).unstack('newlevel')\n",
    "    \n",
    "    # print(df)\n",
    "    \n",
    "    return fit_object, df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful Functions for the Analysis of the 2.7um work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def file_loader(input_list:list, # a list of file names to load\n",
    "                name_list:list, # a list of what we want the new files to be named\n",
    "                path:str, # a string containing the path to the folder containing the files\n",
    "                skip_val=1, # by default skip first row of the file\n",
    "                data_type='pump-probe', # type of data being loaded\n",
    "               ) -> dict: # returns a dictionary where each key is the name given in name_list and the value is a dataframe with the data\n",
    "    data = {name:{} for name in name_list}\n",
    "    \n",
    "    #bar = alive_it(enumerate(input_list))  # <<-- bar with wrapped items\n",
    "    \n",
    "    print(f'There are a total of {len(input_list)} files in the directory')\n",
    "\n",
    "    with alive_bar(len(input_list),force_tty=True) as bar:\n",
    "        for i,file in enumerate(input_list):  \n",
    "            time.sleep(0.005)\n",
    "    \n",
    "            #print(f'File: {file} loaded')\n",
    "        \n",
    "            df = pd.read_excel(path+file,skiprows=skip_val,header=None)\n",
    "        \n",
    "            df2 = data_reformatter(df)\n",
    "\n",
    "            if data_type=='pump-probe':\n",
    "                t0_loc = (df2['median'] - np.mean(df2['median'][0:10])).abs().argmax()\n",
    "            \n",
    "                t0 = df2['delay'][t0_loc]\n",
    "\n",
    "            else:\n",
    "                t0_loc = np.argmax(np.abs(df2['mean']))\n",
    "                t0 = df2['delay'][t0_loc]\n",
    "            \n",
    "            #t0 = df2['delay'][(np.abs(df2['median'][0:len2])).idxmax()]\n",
    "\n",
    "            data[name_list[i]]['t0'] = t0\n",
    "            data[name_list[i]]['df'] = df2\n",
    "            \n",
    "            bar()\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "\n",
    "def basic_plot(data_dict,name_array,**kwargs):\n",
    "\n",
    "    props = dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "\n",
    "    if 'm' in list(kwargs.keys()):\n",
    "        m = kwargs['m']\n",
    "    else:\n",
    "        m = 1\n",
    "\n",
    "    if 'lines' in list(kwargs.keys()):\n",
    "        if kwargs['lines']=='lines':\n",
    "            plt.rc('lines', linewidth=1, marker='')\n",
    "        elif kwargs['lines']=='markers':\n",
    "            plt.rc('lines', linewidth=0, marker='o',markerfacecolor='white')\n",
    "        else:\n",
    "            plt.rc('lines', linewidth=1, marker='o',markerfacecolor='white')\n",
    "            # plt.rc('markers', marker='o',markerfacecolor='white')\n",
    "    else:\n",
    "        marker='o'\n",
    "        \n",
    "    fig,ax = plt.subplots()\n",
    "    \n",
    "    for name in name_array:\n",
    "        df = treatment(data_dict[name])\n",
    "        (xm,ym) = rebin(df['delay'],df['median'],m)\n",
    "        df_new = pd.DataFrame({'delay':xm,'mean':ym})\n",
    "        delay = df_new['delay']\n",
    "        signal = df_new['mean'].to_numpy()\n",
    "        offset = np.mean(signal[0:int(20/m)])\n",
    "        signal -= offset\n",
    "        norm_signal = signal/np.max(np.abs(signal))\n",
    "        xoffset = delay[np.argmax(np.abs(np.gradient(savgol_filter(norm_signal,20,5))))]\n",
    "        ### Assume normal situation is to have sign negative on these traces\n",
    "        sign = int(np.sign(signal[np.argmax(np.abs(signal))]))\n",
    "        if sign==1:\n",
    "            new_sign=-1\n",
    "        else:\n",
    "            new_sign=1\n",
    "        ax.plot(delay-xoffset,new_sign*norm_signal,label=name)\n",
    "        if 'errs' in list(kwargs.keys()):\n",
    "            ax.plot(delay-xoffset,new_sign*norm_signal,label=name)\n",
    "        print(f'The time resolution is: {round(1000*(delay[1]-delay[0]),3)} fs') \n",
    "    \n",
    "        # plt.yscale('semilog')\n",
    "        plt.legend(frameon=False)\n",
    "        ax.set_xlabel(r'$\\mathrm{t_{delay}}$ [ps]')\n",
    "        ax.set_ylabel(r'$\\mathrm{\\Delta \\sigma}$($\\mathrm{t_{delay}}$)')\n",
    "        if 'xlims' in list(kwargs.keys()):\n",
    "            (xmin,xmax) = kwargs['xlims']\n",
    "            ax.set_xlim(xmin,xmax)\n",
    "        if 'ylims' in list(kwargs.keys()):\n",
    "            (ymin,ymax) = kwargs['xlims']\n",
    "            ax.set_ylim(ymin,ymax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def prepulse_analyser(dict_input):\n",
    "\n",
    "    df_input = dict_input['df']\n",
    "\n",
    "    t0 = dict_input['t0']\n",
    "\n",
    "    peak_pos = np.argmax(np.abs(dict_input['df']['mean']-dict_input['df']['mean'][0]))\n",
    "    \n",
    "    t_start_point = 0 #np.where(np.abs(df_input['mean']-trange[0])<0.01)[0][0]\n",
    "    t_end_point = peak_pos# np.where(np.abs(df_input['mean']-trange[1])<0.01)[0][0]\n",
    "\n",
    "    # subrange = np.arange(t_start_point,t_end_point,1)\n",
    "\n",
    "    \n",
    "\n",
    "    scans = [col for col in dict_input['df'] if col.startswith('scan')]\n",
    "\n",
    "    tdata = {'delay':df_input['delay'] - t0}\n",
    "    \n",
    "    df_new = pd.DataFrame(tdata)\n",
    "    \n",
    "    for scan in scans:\n",
    "\n",
    "        y_offset = np.mean(df_input[scan])#np.mean(df_input[scan][int(t_start_point):int(t_end_point)])\n",
    "\n",
    "        df_new[scan] = df_input[scan] - y_offset\n",
    "\n",
    "    scan_select = scans#['scan'+str(i) for i in [1,2,3,4,5,6,7,8,9]]\n",
    "    df_new['mean'] = df_new[scan_select].mean(axis=1)\n",
    "    df_new['median'] = df_new[scan_select].median(axis=1)\n",
    "    df_new['std'] = df_new[scan_select].std(axis=1)\n",
    "    \n",
    "    return df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def linear_spacing(df_input,**kwargs):\n",
    "    \n",
    "    df_output = df_input.copy()\n",
    "\n",
    "    if 'col' in list(kwargs.items()):\n",
    "        scans = [col for col in df_output if col.startswith(kwargs['col'])]\n",
    "    else:\n",
    "        scans = [col for col in df_output if col.startswith('median')]\n",
    "\n",
    "    delay = df_output['delay'].to_numpy()\n",
    "    delay_shift = np.roll(delay,1)\n",
    "    t_min = np.min(delay[1:] - delay_shift[1:])\n",
    "    t_max = np.max(delay[1:] - delay_shift[1:])\n",
    "    \n",
    "    new_delay = np.arange(delay[0],delay[len(delay)-1],t_min)\n",
    "    \n",
    "    dict_new = {}\n",
    "    dict_new['delay'] = new_delay\n",
    "\n",
    "    for scan in scans:\n",
    "        dict_new[scan] = np.interp(new_delay,delay,df_output[scan])\n",
    "\n",
    "    df_out = pd.DataFrame(dict_new)\n",
    "\n",
    "    print(t_min,t_max)\n",
    "\n",
    "    return df_out "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def treatment(dict_input):\n",
    "    df_input = prepulse_analyser(dict_input) # removes the offset\n",
    "    df_output = mean_spike(df_input) # calculates the mean, assuming there are some spikes in the data\n",
    "    return df_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def mean_spike(df_input):\n",
    "\n",
    "    df_output = df_input.copy()\n",
    "\n",
    "    scans = [col for col in df_output if col.startswith(\"scan\")]\n",
    "\n",
    "    mean_val = df_output['mean']\n",
    "\n",
    "    std_val = df_output['std']\n",
    "\n",
    "    N = len(mean_val)\n",
    "\n",
    "    # def spike(x,mval,sval):\n",
    "    #     for i in range(len(x)):\n",
    "    #         if (np.abs(x[i]-mval[i])<(3*sval[i])):\n",
    "    #             val_total = x\n",
    "    \n",
    "    \n",
    "    spike_removed_mean = np.zeros(N)\n",
    "    spike_removed_std = np.zeros(N)\n",
    "\n",
    "    for i in range(N):\n",
    "        dummy = []\n",
    "        for j,scan in enumerate(scans):\n",
    "            if ((np.abs(df_output[scan][i]-mean_val[i]))<(3*std_val[i])):\n",
    "                dummy.append(df_output[scan][i])\n",
    "            else:\n",
    "                pass\n",
    "\n",
    "        spike_removed_mean[i] = np.mean(dummy)\n",
    "        spike_removed_std[i] = np.std(dummy)\n",
    "    df_output['mean_spike'] = spike_removed_mean\n",
    "    df_output['std_spike'] = spike_removed_std\n",
    "\n",
    "    return df_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plotting_global(named_function,fit_tuple,name_array,data,**kwargs):\n",
    "\n",
    "    (fit_obj,df) = fit_tuple\n",
    "    \n",
    "    font = {'family' : 'sans-serif',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : 12}\n",
    "\n",
    "    mpl.rc('font', **font)\n",
    "\n",
    "    colors = plt.cm.jet(np.linspace(0,1,len(name_array)))\n",
    "\n",
    "    fig = plt.figure(figsize=(6,6))\n",
    "    gs = fig.add_gridspec(2, hspace=0, height_ratios=[1, 4])\n",
    "    axs = gs.subplots(sharex=True)\n",
    "\n",
    "    axs[0].set_ylim([-1e-2,1e-2])\n",
    "    if 'xlims' in list(kwargs.keys()):\n",
    "        xmin,xmax = kwargs['xlims']\n",
    "        axs[1].set_xlim([xmin,xmax])\n",
    "\n",
    "    if 'ylims' in list(kwargs.keys()):\n",
    "        ymin,ymax = kwargs['ylims']\n",
    "        axs[1].set_ylim([ymin,ymax])\n",
    "\n",
    "    if 'names' in list(kwargs.keys()):\n",
    "        new_names = kwargs['names']\n",
    "    else:\n",
    "        new_names = name_array\n",
    "\n",
    "    axs[0].set_xlabel(r'delay [ps]')\n",
    "    axs[1].set_ylabel(r'$\\Delta T / T$')\n",
    "\n",
    "    df_names = {name:{} for name in name_array}\n",
    "    \n",
    "    for name in name_array:\n",
    "        df_names[name] = treatment(data[name])\n",
    "\n",
    "    for i,name in enumerate(name_array):\n",
    "        offset = df_names[name]['median'][0:30].mean(axis=0)\n",
    "        mean_sig = df_names[name]['median']-offset\n",
    "        std_sig = df_names[name]['std_spike']\n",
    "        delay1 = df_names[name]['delay']\n",
    "\n",
    "        if np.abs(np.min(mean_sig))>np.abs(np.max(mean_sig)):\n",
    "            signage = -1\n",
    "            fit_obj.popt[i,0]=fit_obj.popt[i,0]\n",
    "            # fit_obj.popt[i,-1]=0\n",
    "        else:\n",
    "            signage = 1\n",
    "        \n",
    "        dif = mean_sig-signage*named_function(delay1,*fit_obj.popt[i,:])\n",
    "        \n",
    "        axs[1].plot(delay1,mean_sig,color = colors[i],marker='o',linewidth=0,markerfacecolor='white',label=new_names[i])\n",
    "        # axs[1].fill_between(delay,\n",
    "        #                     signage*mean_sig+std_sig,\n",
    "        #                     signage*mean_sig-std_sig,color=colors[i],alpha=0.2)\n",
    "\n",
    "        # print(fit_obj.popt[i,:])\n",
    "        # print(name,fit_obj.popt[0])\n",
    "        axs[1].plot(delay1,signage*named_function(delay1,*fit_obj.popt[i,:]),color = colors[i],linewidth=3)\n",
    "\n",
    "        axs[0].plot(delay1,dif,color = colors[i],marker='o',linewidth=0,markerfacecolor='white')\n",
    "\n",
    "        axs[1].set_xlabel('delay [ps]')\n",
    "        # axs[0].fill_between(delay,\n",
    "        #                     dif+std_sig,\n",
    "        #                     dif-std_sig,color=colors[i],alpha=0.2)\n",
    "\n",
    "    plt.legend(frameon=False)\n",
    "\n",
    "    if 'save' in list(kwargs.keys()):\n",
    "        filename_plot = './Presentation/images/'+kwargs['save']+'.svg'\n",
    "        filename_table = './Presentation/tables/'+kwargs['save']+'.pkl'\n",
    "        os.makedirs(os.path.dirname(filename_plot), exist_ok=True)\n",
    "        os.makedirs(os.path.dirname(filename_table), exist_ok=True)\n",
    "        fig.savefig(filename_plot,bbox_inches='tight')\n",
    "        df.to_pickle(filename_table)\n",
    "    return 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def global_plot(name_dict,data,guess,**kwargs):\n",
    "\n",
    "    df_names = {name:{} for name in name_dict}\n",
    "    \n",
    "    for name in name_dict:\n",
    "        df_names[name] = treatment(data[name])\n",
    "\n",
    "    new_x = {}\n",
    "    new_y= {}\n",
    "    \n",
    "    for i,name in enumerate(name_dict):\n",
    "        offset = df_names[name]['median'][0:30].mean(axis=0) # y offset\n",
    "\n",
    "        ### experimental\n",
    "        df_linear =  linear_spacing(df_names[name],col='median')\n",
    "        mean_sig = df_linear['median']\n",
    "        delay = df_linear['delay']\n",
    "        \n",
    "        # mean_sig = df_names[name]['median']\n",
    "        # delay = df_names[name]['delay'].copy()\n",
    "\n",
    "        if np.abs(np.min(mean_sig-offset))>np.abs(np.max(mean_sig-offset)):\n",
    "            signage = -1\n",
    "        else:\n",
    "            signage = 1\n",
    "\n",
    "        yval = mean_sig-offset\n",
    "        \n",
    "        if 'trange' in list(kwargs.keys()):\n",
    "            trange=kwargs['trange']\n",
    "            if type(trange)==tuple:\n",
    "                tranger = trange\n",
    "            elif type(trange)==dict:\n",
    "                tranger = trange[name]\n",
    "                \n",
    "            tol = np.abs(delay[1]-delay[0])\n",
    "            t_start = np.where(np.abs(delay-tranger[0])<tol)[0][0]\n",
    "            t_end = np.where(np.abs(delay-tranger[1])<tol)[0][0]\n",
    "            t_start = 0\n",
    "            delay = delay[t_start:t_end]\n",
    "            yval = yval[t_start:t_end]\n",
    "                \n",
    "\n",
    "        if 'xlims' in list(kwargs.keys()):\n",
    "            pass\n",
    "        if 'ylims' in list(kwargs.keys()):\n",
    "            pass\n",
    "        if 'save' in list(kwargs.keys()):\n",
    "            pass\n",
    "        \n",
    "        new_x[name] = delay\n",
    "        new_y[name] = signage*yval\n",
    "\n",
    "    fit_tuple = global_fit(convolved_biexp_decay_beta, new_x, new_y, guess)\n",
    "\n",
    "    (fig,df) = fit_tuple\n",
    "\n",
    "    col_names = [s for s in df.columns if s.startswith('trace')]\n",
    "\n",
    "    new_col_dict = dict(zip(col_names,name_dict))\n",
    "    df.rename(columns=new_col_dict,inplace=True)\n",
    "\n",
    "    tuple_new = (fig,df)\n",
    "\n",
    "    plotting_global(convolved_biexp_decay_beta,fit_tuple,name_dict,data,**kwargs)\n",
    "    \n",
    "    return tuple_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def auto_guess(dict_1,nos_traces):\n",
    "\n",
    "    ([*keys],vals) = zip(*dict_1.items())\n",
    "    new_dict = {}\n",
    "    for i in range(nos_traces):\n",
    "        for j in range(len(keys)):\n",
    "            new_keys = keys[j]+str(i)\n",
    "            new_dict[new_keys] = vals[j]\n",
    "\n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def file_save(save_name,fig,**kwargs):\n",
    "    dpi=None\n",
    "    if 'ext' in list(kwargs.keys()):\n",
    "        filename_plot = './Presentation/images/'+save_name+'.'+kwargs['ext']\n",
    "        if kwargs['ext']=='png':\n",
    "            dpi = 600\n",
    "    else:\n",
    "        filename_plot = './Presentation/images/'+save_name+'.svg'\n",
    "\n",
    "    os.makedirs(os.path.dirname(filename_plot), exist_ok=True)\n",
    "    fig.savefig(filename_plot, bbox_inches='tight', dpi=dpi, format=kwargs['ext'])\n",
    "    \n",
    "    if 'df' in list(kwargs.keys()):\n",
    "        filename_table = './Presentation/tables/'+save_name+'.pkl'\n",
    "        os.makedirs(os.path.dirname(filename_table), exist_ok=True)\n",
    "        kwargs['df'].to_pickle(filename_table)\n",
    "\n",
    "    return 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def resid_only(named_function,fit_tuple,name_array_sub,full_name_array,data,**kwargs):\n",
    "\n",
    "    ''' Common to all code '''\n",
    "    font = {'family' : 'sans-serif',\n",
    "        'weight' : 'regular',\n",
    "        'size'   : 12}\n",
    "\n",
    "    mpl.rc('font', **font)\n",
    "\n",
    "    colors = ['red','black','green','blue','purple','grey','lightgreen']#plt.cm.jet(np.linspace(0,1,len(name_array_sub)))\n",
    "    \n",
    "    df_names = {name:{} for name in name_array_sub}\n",
    "\n",
    "    print(name_array_sub)\n",
    "    for name in name_array_sub:\n",
    "        df_names[name] = treatment(data[name])\n",
    "\n",
    "    index_pos = [full_name_array.index(x) for x in name_array_sub if x in full_name_array]\n",
    "\n",
    "    fig,ax = plt.subplots()\n",
    "    \n",
    "    if 'xlims' in list(kwargs.keys()):\n",
    "        xlims = kwargs['xlims']\n",
    "        ax.set_xlim(*xlims)\n",
    "    else:\n",
    "        ax.set_xlim([0,20])\n",
    "        \n",
    "    if 'ylims' in list(kwargs.keys()):\n",
    "        ylims = kwargs['ylims']\n",
    "        ax.set_ylim(*ylims)\n",
    "\n",
    "    if 'names' in list(kwargs.keys()):\n",
    "        names = kwargs['names']\n",
    "    else:\n",
    "        names = name_array_sub\n",
    "\n",
    "    dict_out = {}\n",
    "    R2_arr = []\n",
    "    (df,fit_obj) = fit_tuple\n",
    "    for i,name in enumerate(name_array_sub):\n",
    "        offset = df_names[name]['median'][0:30].mean(axis=0)\n",
    "        mean_sig = df_names[name]['median']-offset\n",
    "        std_sig = df_names[name]['std_spike']\n",
    "        delay1 = df_names[name]['delay']\n",
    "\n",
    "        resid = mean_sig - named_function(delay1,*fit_obj.popt[index_pos[i]])\n",
    "        # ax.plot(delay1,resid,linewidth=0.5,color=colors[i],label=name)\n",
    "        ax.set_xlabel('time [ps]')\n",
    "        ax.set_ylabel('residual')\n",
    "\n",
    "        if 'histogram' in list(kwargs.keys()):\n",
    "            df_hist = pd.DataFrame({'val':mean_sig.to_numpy(),'delay':delay1}) \n",
    "            sns.displot(df_hist, x=\"val\", binwidth=0.0005)\n",
    "        \n",
    "        if 'fit' in list(kwargs.keys()):\n",
    "            named_function2 = kwargs['fit']\n",
    "            if 'guess' in list(kwargs.keys()):\n",
    "                guess = kwargs['guess']\n",
    "            else:\n",
    "                if named_function2==cos_fn:            \n",
    "                    guess = [0.16,np.max(resid),0,.001]\n",
    "                if named_function2==cos_fn2:\n",
    "                    guess = [0.08,0.8*np.max(resid),np.max(resid)*0.2,0.1,.001]\n",
    "                if named_function2==cos_fn3:\n",
    "                    guess = [0.08,0.6*np.max(resid),0.3*np.max(resid),0.1*np.max(resid),0.1,.001]\n",
    "            tol=np.abs(delay1[1]-delay1[0])\n",
    "            t_start_pt = np.where(np.abs(delay1-xlims[0])<tol)[0][0]\n",
    "            t_end_pt = np.where(np.abs(delay1-xlims[1])<tol)[0][0]\n",
    "            popt,pcov = curve_fit(named_function2,delay1[t_start_pt:t_end_pt],resid[t_start_pt:t_end_pt],p0=guess,method='lm')\n",
    "            offset = popt[len(popt)-1]\n",
    "            \n",
    "            ax.plot(delay1,resid-offset,linewidth=0.1,alpha=1,color=colors[i])\n",
    "            ax.plot(delay1,resid-offset,marker='o',linewidth=0,markerfacecolor='white',color=colors[i],label=names[i])\n",
    "            \n",
    "            \n",
    "            ax.plot(delay1,named_function2(delay1,*popt)-offset,color=colors[i],linewidth=2)\n",
    "            fit_obj2 = fit_result(delay1[t_start_pt:t_end_pt],resid[t_start_pt:t_end_pt],named_function2,popt,pcov)\n",
    "            # print(popt)\n",
    "            variables = inspect.getfullargspec(named_function2).args\n",
    "            variables = variables[1:]\n",
    "            #outputs = [names[i]]\n",
    "            dict_out[names[i]] = zip(np.round(fit_obj2.popt,3),np.round(fit_obj2.pstd,3))\n",
    "            R2_arr.append(fit_obj2.R2)\n",
    "            \n",
    "            print(f'R2 = {round(fit_obj2.R2_adj,3)}')\n",
    "        else:\n",
    "            ax.plot(delay1,resid,linewidth=0.1,alpha=1,color=colors[i])\n",
    "            ax.plot(delay1,resid,marker='o',linewidth=0,markerfacecolor='white',color=colors[i],label=names[i])\n",
    "        plt.legend(frameon=False,loc='upper left')\n",
    "\n",
    "    print(dict_out)\n",
    "    df_out = pd.DataFrame(dict_out,index=variables)\n",
    "    df_out.loc['R2'] = R2_arr\n",
    "        \n",
    "    if 'save' in list(kwargs.keys()):\n",
    "        save_name = kwargs['save']\n",
    "        file_save(save_name,fig,df=df_out)\n",
    "\n",
    "    print(df_out)\n",
    "    return df_out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def cos_fn(x,f,A,x0,y0):\n",
    "    # A,f,phi,y0 = args\n",
    "    return A*np.cos(2*np.pi*f*(x-x0))+y0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for use with un-excited THz-TDS data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def electric_field(input_data,\n",
    "                   conversion_scale, #from oscilloscope magnitude to LIA reading\n",
    "                   lambda0, # in nm\n",
    "                   Si_in=True):\n",
    "\n",
    "    n0 = 2.7830\n",
    "    n_air = 1.0\n",
    "    r41 = 4.04e-12 # m/V \n",
    "    L = 400e-6 # m\n",
    "    lambda0 *= 1e-9 # m\n",
    "    t_refl = (n0 - n_air)/(n0 + n_air)\n",
    "\n",
    "    E_out_scale = lambda0 / (2*np.pi*n0**3*r41*t_refl*L)\n",
    "\n",
    "    E_out = np.arcsin(input_data*1e-3*conversion_scale/12) * E_out_scale\n",
    "\n",
    "    scale=1\n",
    "    if Si_in==True:\n",
    "        scale = 0.8\n",
    "\n",
    "    return (E_out/scale)/1e5 # kV/cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def data_reformatter(df,**kwargs):\n",
    "\n",
    "    \"\"\"\n",
    "    The purpose of this function is to repackage or change the format of the data, \n",
    "    such that rather than having two columns with a large number of rows, we have\n",
    "    different columns for each scan. The data is also augmented with two additional \n",
    "    rows - the mean and the std of the scans.\n",
    "\n",
    "    kwargs here are used to either correct result to E(kV) or not\n",
    "    \"\"\"\n",
    "    if kwargs['convert_to_E']==True:\n",
    "        scale_LIA = kwargs['conversion_scale']\n",
    "        lambda0 = kwargs['lambda0']\n",
    "        Si_in = kwargs['Si_in']\n",
    "    else:\n",
    "        scale_LIA = 1.0\n",
    "        \n",
    "    data = df.to_numpy()\n",
    "\n",
    "    Ntot,_ = np.shape(data) # total number of rows\n",
    "\n",
    "    delays = np.unique(data[:,0]) # unique delay positions in mm\n",
    "    \n",
    "    delays = 2*delays/(consts.c) * 1e-3 * 1e12\n",
    "    \n",
    "    N = len(delays) # number of delays per scan\n",
    "\n",
    "    M = int(Ntot/N) # number of scans\n",
    "\n",
    "    df2 = pd.DataFrame(np.reshape(data[:,1],(M,N)).T) # reshape the data and put it into a dataframe\n",
    "\n",
    "    col_names = {i:f'scan{i}' for i in range(M)} # create an array with names like 'scan0, scan1, ....'\n",
    "    \n",
    "    df2 = df2.rename(columns=col_names) # rename the columns\n",
    "\n",
    "    for col in col_names:\n",
    "        df2[col] = electric_field(df2[col],\n",
    "                                  scale_LIA,\n",
    "                                  lambda0,\n",
    "                                  Si_in)\n",
    "\n",
    "    mean_col = df2.mean(axis=1) # calculate a new column containing the mean value for each delay\n",
    "\n",
    "    median_col = df2.median(axis=1)\n",
    "\n",
    "    std_col = df2.std(axis=1) # calculate a new column containing the std value for each delay\n",
    "\n",
    "    df2['mean'] = mean_col # augment the dataframe with a new column containing the mean\n",
    "\n",
    "    df2['median'] = median_col\n",
    "    \n",
    "    df2['std'] = std_col # augment the dataframe with a new column containing the std\n",
    "\n",
    "    df2['se'] = std_col / np.sqrt(M)\n",
    "    \n",
    "    df2.insert(loc=0,column='delay',value=delays) # add a new column at the start of the DataFrame containing the delays\n",
    "\n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def fft_simple(df,**kwargs):\n",
    "    \"\"\"\n",
    "    This is a simple implementation of fft that will work for a DataFrame object\n",
    "    \"\"\"\n",
    "    # temporal resolution\n",
    "    dt = np.abs(df['delay'][1] - df['delay'][0])\n",
    "    \n",
    "    # sampling rate\n",
    "    S = 1/dt\n",
    "\n",
    "    pad = kwargs['pad']\n",
    "\n",
    "    N_old,M = np.shape(df)\n",
    "\n",
    "    scan_cols = [s for s in df if s.startswith('scan')]\n",
    "    \n",
    "    M = len(scan_cols)\n",
    "    \n",
    "    scans = df[scan_cols].to_numpy()\n",
    "\n",
    "    zero_pad = np.zeros((pad,M))\n",
    "\n",
    "    data_pad = np.vstack((scans,zero_pad)) # just stacking at end, should probably zero pad both sides\n",
    "\n",
    "    ##t_pad = np.array([df['delay'][0] + dt*i for i in range(N_old+pad)])\n",
    "\n",
    "    freqs = fftfreq(N_old+pad,dt)\n",
    "    \n",
    "    # FFT of signal\n",
    "    N_freq = len(freqs)\n",
    "    \n",
    "    N2 = int(np.floor(N_freq/2))\n",
    "\n",
    "    signal = fftn(data_pad,axes=0)\n",
    "\n",
    "    PSD = np.zeros(np.shape(signal))\n",
    "\n",
    "    arg_cmplx = np.zeros(np.shape(signal))\n",
    "    \n",
    "    for i in range(np.size(signal,1)):\n",
    "        PSD[:,i] = np.abs(signal[:,i])**2\n",
    "        arg_cmplx[:,i] = np.unwrap(np.angle(signal[:,i]))\n",
    "\n",
    "    df2 = pd.DataFrame(signal[1:N2])\n",
    "\n",
    "    col_names = {i:f'scan{i}' for i in range(M)}\n",
    "\n",
    "    cols = ['scan'+str(i) for i in range(M)]\n",
    "    \n",
    "    df2.rename(columns=col_names,inplace=True)\n",
    "\n",
    "    mean_col = df2.mean(axis=1)\n",
    "\n",
    "    std_col = np.std(df2.to_numpy(),axis=1)\n",
    "\n",
    "    df2.insert(loc=0,column='freq',value=freqs[1:N2])\n",
    "\n",
    "    df2['mean'] = mean_col\n",
    "\n",
    "    df2['std'] = std_col\n",
    "\n",
    "    df2['PSD'] = np.mean(PSD[1:N2],axis=1)\n",
    "\n",
    "    df2['PSD_err'] = np.std(PSD[1:N2],axis=1)\n",
    "\n",
    "    df2['arg'] = np.mean(arg_cmplx[1:N2],axis=1)\n",
    "\n",
    "    df2['arg_err'] = np.std(arg_cmplx[1:N2],axis=1)\n",
    "    \n",
    "    return df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def transmission(df1,df2):\n",
    "\n",
    "    cols = [s for s in df1 if s.startswith('scan')]\n",
    "    \n",
    "    M = len(cols)\n",
    "\n",
    "    col_names = {i:'scan'+str(i) for i in range(M)}\n",
    "    \n",
    "    df_T = df1[cols].to_numpy()\n",
    "\n",
    "    ref_mean = df2['mean'].to_numpy()\n",
    "\n",
    "    dnew = np.zeros((np.size(df_T,0),M),dtype=np.complex128)\n",
    "    \n",
    "    for i in range(M):\n",
    "\n",
    "        temp = df_T[:,i]/ref_mean\n",
    "        \n",
    "        tmod = np.abs(df_T[:,i]/ref_mean)\n",
    "                \n",
    "        dnew[:,i] = temp\n",
    "\n",
    "    T_mean = np.mean(dnew,axis=1)\n",
    "    \n",
    "    T_std = np.std(dnew,axis=1)\n",
    "    \n",
    "    df = pd.DataFrame(dnew)\n",
    "\n",
    "    df.rename(columns=col_names,inplace=True)\n",
    "\n",
    "    df['T'] = T_mean\n",
    "\n",
    "    df['T_err'] = T_std\n",
    "    \n",
    "    df.insert(loc=0,column='freq',value=df1['freq'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def conductivity(T_in,**kwargs):\n",
    "    \"\"\"\n",
    "    This function calculates the complex conductivity according to equation 1 or 2\n",
    "    \"\"\"\n",
    "    n_substrate = 3.45+0.25j # Assuming a constant, real refractive index for the substrate. This is not true, but need to dig up the measured data.\n",
    "    Z_0 = 377 # ohms, vacuum permittivity\n",
    "    n_A = n_substrate + 1 # from equation section\n",
    "    n_B = n_substrate + 1 # from equation section\n",
    "    e2_h = consts.e**2/consts.h # factor that converts to units of e^2/h\n",
    "    \n",
    "    cols = [s for s in T_in if s.startswith('scan')]\n",
    "    \n",
    "    M = len(cols)\n",
    "\n",
    "    col_names = {i:'scan'+str(i) for i in range(M)}\n",
    "    \n",
    "    data_in = T_in[cols].to_numpy()\n",
    "\n",
    "    dnew = np.zeros((np.size(data_in,0),M),dtype=np.complex128)\n",
    "\n",
    "    if (kwargs[\"equation\"]==\"eq1\"):\n",
    "        \n",
    "        for i in range(M):\n",
    "            \n",
    "            dnew[:,i] = (1/e2_h) * n_A/Z_0 * ((1/data_in[:,i]) - 1)\n",
    "\n",
    "    if (kwargs[\"equation\"]==\"eq2\"):\n",
    "\n",
    "        for i in range(M):\n",
    "            \n",
    "            dnew[:,i] = (1/e2_h) * (n_A * np.sqrt(n_A**2 + 4*n_B*(n_A+n_B)*data_in[:,i]) - \\\n",
    "            n_A**2 - 2*n_A*n_B*data_in[:,i])/(2*n_B*Z_0*data_in[:,i])\n",
    "            \n",
    "    sigma_mean = np.mean(dnew,axis=1)\n",
    "    \n",
    "    sigma_std = np.std(dnew,axis=1)\n",
    "    \n",
    "    df = pd.DataFrame(dnew)\n",
    "\n",
    "    df.rename(columns=col_names,inplace=True)\n",
    "\n",
    "    df['mean'] = sigma_mean\n",
    "\n",
    "    df['std'] = sigma_std\n",
    "\n",
    "    df['real'] = np.mean(np.real(dnew),axis=1)\n",
    "\n",
    "    df['real_err'] = np.std(np.real(dnew),axis=1)\n",
    "\n",
    "    df['imag'] = np.mean(np.imag(dnew),axis=1)\n",
    "\n",
    "    df['imag_err'] = np.std(np.imag(dnew),axis=1)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def four_point(x:np.ndarray, # Input array that contains the independent variable.\n",
    "    mu_e:np.double, # electron mobility\n",
    "    mu_h:np.double, # hole mobility\n",
    "    dn:np.double, # non-zero conductivity at CNP\n",
    "    rho_s:np.double, # Gate indpendent resistivity given by short range scattering in graphene\n",
    "    n_0:np.double, # carrier density at Vg=0 (intrinsic doping)\n",
    "    **kwargs) ->np.ndarray: # A new float array containing the conductivity.\n",
    "    #J. H. Gosling, O. Makarovsky, F. Wang, et al., “Universal mobility characteristics of graphene originating from charge scattering by ionised impurities,” Commun. Phys. 4 (2021).593\n",
    "\n",
    "    ## Constants used ##\n",
    "    e = 1.6e-19 # electron charge\n",
    "    eps_0 = 8.8541878188e-12 # F/m, vacuum permittivity\n",
    "    A = 7.0 * 7.0 # mm2, device area\n",
    "    d = 300 # nm, SiO2 thickness\n",
    "    eps_sio2 = 3.9 # dielectric constant of SiO2\n",
    "    C = eps_sio2 * eps_0 * A * 1e-6 / (d*1e-9) # capacitance in Farads, F\n",
    "    \n",
    "    n_C = C * x / e # geometric carrier concentration using parralel plate capacitance model\n",
    "\n",
    "    V_CNP = kwargs['CNP'] # not sure if this bit of code is going to work\n",
    "\n",
    "    if (x < V_CNP):\n",
    "        \n",
    "        mu = mu_h\n",
    "        \n",
    "    elif (x > V_CNP):\n",
    "        \n",
    "        mu = mu_e\n",
    "\n",
    "    n_carrier = np.abs(((1/n_C) + (e*mu/rho_s))**(-1) + n_0)\n",
    "\n",
    "    \n",
    "    if (n_carrier < (dn/2)):\n",
    "\n",
    "        sigma = e * mu * (dn/4 + (n_carrier**2)/dn)\n",
    "\n",
    "    elif (n_carrier > (dn/2)):\n",
    "\n",
    "        sigma = e * mu * (dn/4 + (n_carrier**2)/dn)        \n",
    "\n",
    "\n",
    "    return sigma\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def foo(): pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def file_save(save_name,fig,**kwargs):\n",
    "    if 'file_type' in list(kwargs.keys()):\n",
    "        filename_plot = './Presentation/images/'+save_name+kwargs['file_type']\n",
    "    else:\n",
    "        filename_plot = './Presentation/images/'+save_name+'.svg'\n",
    "    \n",
    "    if 'df' in list(kwargs.keys()):\n",
    "        filename_table = './Presentation/tables/'+save_name+'.pkl'\n",
    "        os.makedirs(os.path.dirname(filename_table), exist_ok=True)\n",
    "        kwargs['df'].to_pickle(filename_table)\n",
    "\n",
    "    os.makedirs(os.path.dirname(filename_plot), exist_ok=True)\n",
    "    \n",
    "    fig.savefig(filename_plot,bbox_inches='tight')\n",
    "    \n",
    "    print(f'Figure saved successfully at {filename_plot}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
